name: Deploy to EKS

on:
  workflow_run:
    workflows: ["Build and Push Images"]
    types:
      - completed
    branches:
      - main
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      image_tag:
        description: 'Image tag to deploy (defaults to commit SHA)'
        required: false
        default: ''

env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: video-pipeline-cluster
  # Dual-stream Kafka topics
  KAFKA_TOPICS: "video-frames-1,video-frames-2"

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    permissions:
      id-token: write
      contents: read
    environment: ${{ github.event.inputs.environment || 'dev' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Get AWS resource values
        id: tf-outputs
        run: |
          # Get MSK bootstrap servers
          MSK_CLUSTER_ARN=$(aws kafka list-clusters --query 'ClusterInfoList[?ClusterName==`video-pipeline-kafka`].ClusterArn' --output text)
          if [ -n "$MSK_CLUSTER_ARN" ]; then
            KAFKA_BOOTSTRAP=$(aws kafka get-bootstrap-brokers --cluster-arn $MSK_CLUSTER_ARN --query 'BootstrapBrokerString' --output text)
            echo "kafka_bootstrap=$KAFKA_BOOTSTRAP" >> $GITHUB_OUTPUT
          fi
          
          # Get BOTH S3 buckets for dual-stream support
          # Bucket 1: video-pipeline-output-1-*
          S3_BUCKET_1=$(aws s3api list-buckets --query 'Buckets[?starts_with(Name, `video-pipeline-output-1`)].Name' --output text | head -1)
          echo "s3_bucket_1=$S3_BUCKET_1" >> $GITHUB_OUTPUT
          
          # Bucket 2: video-pipeline-output-2-*
          S3_BUCKET_2=$(aws s3api list-buckets --query 'Buckets[?starts_with(Name, `video-pipeline-output-2`)].Name' --output text | head -1)
          echo "s3_bucket_2=$S3_BUCKET_2" >> $GITHUB_OUTPUT
          
          # Get consumer IAM role
          CONSUMER_ROLE_ARN=$(aws iam get-role --role-name video-pipeline-consumer-pod --query 'Role.Arn' --output text 2>/dev/null || echo "")
          echo "consumer_role_arn=$CONSUMER_ROLE_ARN" >> $GITHUB_OUTPUT
          
          echo "Found S3 buckets: $S3_BUCKET_1, $S3_BUCKET_2"

      - name: Update secrets
        env:
          KAFKA_BOOTSTRAP: ${{ steps.tf-outputs.outputs.kafka_bootstrap }}
          S3_BUCKET_1: ${{ steps.tf-outputs.outputs.s3_bucket_1 }}
          S3_BUCKET_2: ${{ steps.tf-outputs.outputs.s3_bucket_2 }}
        run: |
          kubectl create namespace video-pipeline --dry-run=client -o yaml | kubectl apply -f -
          
          # Create secrets with BOTH S3 buckets for dual-stream
          kubectl create secret generic video-pipeline-secrets \
            --namespace video-pipeline \
            --from-literal=KAFKA_BOOTSTRAP_SERVERS="${KAFKA_BOOTSTRAP}" \
            --from-literal=S3_BUCKET_1="${S3_BUCKET_1}" \
            --from-literal=S3_BUCKET_2="${S3_BUCKET_2}" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Determine image tag
        id: image-tag
        run: |
          # Use commit SHA from triggering workflow, manual input, or current SHA
          if [ -n "${{ github.event.inputs.image_tag }}" ]; then
            TAG="${{ github.event.inputs.image_tag }}"
          elif [ -n "${{ github.event.workflow_run.head_sha }}" ]; then
            TAG="${{ github.event.workflow_run.head_sha }}"
          else
            TAG="${{ github.sha }}"
          fi
          echo "tag=$TAG" >> $GITHUB_OUTPUT
          echo "Using image tag: $TAG"

      - name: Check which images exist
        id: check-images
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ steps.image-tag.outputs.tag }}
        run: |
          # Check if each image exists with the commit SHA tag
          # If not, fall back to 'latest'
          
          for service in inference consumer producer; do
            if aws ecr describe-images --repository-name video-pipeline/$service --image-ids imageTag=$IMAGE_TAG 2>/dev/null; then
              echo "${service}_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
              echo "$service: using SHA tag $IMAGE_TAG"
            else
              echo "${service}_tag=latest" >> $GITHUB_OUTPUT
              echo "$service: using latest tag (SHA not found)"
            fi
          done

      - name: Deploy with Kustomize
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          INFERENCE_TAG: ${{ steps.check-images.outputs.inference_tag }}
          CONSUMER_TAG: ${{ steps.check-images.outputs.consumer_tag }}
          CONSUMER_ROLE_ARN: ${{ steps.tf-outputs.outputs.consumer_role_arn }}
        run: |
          cd k8s
          
          echo "Deploying inference:$INFERENCE_TAG, consumer:$CONSUMER_TAG"
          
          # Update image references using kustomize
          kustomize edit set image \
            PLACEHOLDER_REGISTRY/video-pipeline/inference=${ECR_REGISTRY}/video-pipeline/inference:${INFERENCE_TAG} \
            PLACEHOLDER_REGISTRY/video-pipeline/consumer=${ECR_REGISTRY}/video-pipeline/consumer:${CONSUMER_TAG}
          
          # Apply manifests
          kubectl apply -f namespace.yaml
          kubectl apply -f configmap.yaml
          
          # Update consumer service account annotation (Linux sed)
          if [ -n "$CONSUMER_ROLE_ARN" ]; then
            sed -i "s|\${CONSUMER_ROLE_ARN}|${CONSUMER_ROLE_ARN}|g" consumer/deployment.yaml
          fi
          
          # Update image tags per service (Linux sed)
          sed -i "s|\${ECR_REGISTRY}|${ECR_REGISTRY}|g" inference/deployment.yaml
          sed -i "s|\${ECR_REGISTRY}|${ECR_REGISTRY}|g" consumer/deployment.yaml
          sed -i "s|:latest|:${INFERENCE_TAG}|g" inference/deployment.yaml
          sed -i "s|:latest|:${CONSUMER_TAG}|g" consumer/deployment.yaml
          
          # Apply Kubernetes manifests - new image tag triggers automatic pod restart
          kubectl apply -f inference/deployment.yaml
          kubectl apply -f inference/service.yaml
          kubectl apply -f inference/hpa.yaml || true  # HPA with KEDA may fail if KEDA not installed
          kubectl apply -f consumer/

      - name: Wait for deployments
        run: |
          # Wait for deployments (timeout 5 minutes)
          kubectl rollout status deployment/inference-service -n video-pipeline --timeout=300s || true
          kubectl rollout status deployment/consumer -n video-pipeline --timeout=300s || true
          
          # Verify at least one pod is running
          echo "Checking pod status..."
          kubectl get pods -n video-pipeline

      - name: Verify deployment
        run: |
          echo "=== Deployment Status ==="
          kubectl get deployments -n video-pipeline
          
          echo ""
          echo "=== Pod Status ==="
          kubectl get pods -n video-pipeline
          
          echo ""
          echo "=== Services ==="
          kubectl get services -n video-pipeline

      - name: Deploy producer to EC2
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          PRODUCER_TAG: ${{ steps.check-images.outputs.producer_tag }}
        run: |
          # Get EC2 instance ID for RTSP server (runs producer)
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=*rtsp*" "Name=instance-state-name,Values=running" \
            --query 'Reservations[0].Instances[0].InstanceId' --output text)
          
          if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
            echo "Updating producer on EC2 instance: $INSTANCE_ID"
            
            # Use SSM to pull new image and restart producer container
            # Producer publishes to BOTH Kafka topics for dual-stream
            aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters "commands=[
                'aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ${ECR_REGISTRY}',
                'docker pull ${ECR_REGISTRY}/video-pipeline/producer:${PRODUCER_TAG}',
                'docker stop producer || true',
                'docker rm producer || true',
                'docker run -d --name producer --network host -e KAFKA_BOOTSTRAP_SERVERS=\$(aws kafka get-bootstrap-brokers --cluster-arn \$(aws kafka list-clusters --query ClusterInfoList[0].ClusterArn --output text) --query BootstrapBrokerString --output text) -e KAFKA_TOPICS=${KAFKA_TOPICS} -e RTSP_URL=rtsp://localhost:8554/stream ${ECR_REGISTRY}/video-pipeline/producer:${PRODUCER_TAG}'
              ]" \
              --output text
            
            echo "Producer deployment command sent to EC2 (dual-stream: ${KAFKA_TOPICS})"
          else
            echo "No running RTSP EC2 instance found - skipping producer deployment"
          fi

      - name: Run smoke test
        run: |
          # Port forward to inference service
          kubectl port-forward svc/inference-service 8000:8000 -n video-pipeline &
          sleep 5
          
          # Check health endpoint
          curl -sf http://localhost:8000/health || exit 1
          echo "Smoke test passed!"
          
          # Cleanup
          pkill -f "port-forward" || true

